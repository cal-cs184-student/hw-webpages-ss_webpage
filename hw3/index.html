<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
				line-height: 1.5;
			}

			figure {
				text-align: center;
			}

			.equal_3_size {
				width: 100%;
				height: auto;
				max-width: 300px;
				max-height: 350px;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Lato', sans-serif;
				line-height: 1.2;
			}

			p {
				line-height: 1.2;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Shuang Liu</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-ss_webpage/">cal-cs184-student.github.io/hw-webpages-ss_webpage</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		
		<b>Q1: </b>
		Walk through the ray generation and primitive intersection parts of the rendering pipeline.
		<br>
		<b>A1: </b>
		<b>ray generation</b><br>
		Given a pixel coordinate (x, y) in the image plane, we generate a camera ray originating from the camera position and passing through the corresponding point on the sensor plane in camera space. 
		This ray is initially defined in camera space, and we transform it into world space using the camera-to-world (c2w) transformation matrix. <br>
		To estimate the integral of radiance over a pixel, we perform supersampling by generating ns_aa random sample points within the pixel area. 
		For each sample point, we invoke 'generate_ray()' to generate a corresponding ray in world space. 
		We then call 'est_radiance_global_illumination(Ray r)' multiple times to compute the radiance along these sampled rays. 
		The final pixel value is obtained by averaging these radiance estimates, and the result is stored in sampleBuffer. <br>
		<b>primitive intersection</b><br>
		For each generated ray, we check for intersections with scene primitives such as triangles and spheres.<br>
		In the function 'bool Triangle::has_intersection()', we use the Möller-Trumbore algorithm to determine whether the ray intersects the triangle within the valid range r.min_t and r.max_t.
		In the function 'Trianfle::intersect()', If an intersection occurs, we update the intersection record with the corresponding hit information.<br>
		The intersection test for spheres follows a similar approach, using the quadratic equation derived from the implicit sphere equation. If the ray intersects the sphere within the valid range, we update the intersection record accordingly.<br>
		<br>

		<b>Q2: </b>
		Explain the triangle intersection algorithm you implemented in your own words.
		<br>

		<b>A2: </b>
		The Möller-Trumbore algorithm is a fast and efficient method for determining whether a ray intersects a triangle in 3D space. The algorithm works as follows:<br>
		1. Compute the edges of the triangle by subtracting the vertex positions:
		\[
   		E1 = p2 - p1, \quad E2 = p3 - p1
   		\]<br>
		2. Compute the vector from origin of the ray to the triangle vertex:
		\[
   		S = r.o - p1
   		\]<br>
		3. Compute auxiliary vectors using the cross product:  
		\[
   		S1 = cross(r.d, E2), \quad S2 = cross(S, E1)
   		\]<br>
		4. Solve for intersection parameters using the determinant method:  
		\[
		\text{tri\_test} = \frac{1}{\text{dot}(S1, E1)} \times  
		\begin{bmatrix}  
		\text{dot}(S2, E2) \\  
		\text{dot}(S1, S) \\  
		\text{dot}(r.d, S2)  
		\end{bmatrix}
		\]<br>
		5. Extract the intersection parameters:
		\[
   		t = \text{tri\_test}.x, \quad b2 = \text{tri\_test}.y, \quad b3 = \text{tri\_test}.z, \quad b1 = 1 - b2 - b3
   		\]<br>
		6. If \( r.min\_t < t < r.max\_t \) and \( b_1, b_2, b_3 \) are all greater than zero, then the ray intersects the triangle. <br>
		7. If an intersection occurs, update the intersection record with hit information:  
   		```cpp
   		r.max_t = t;
   		isect->t = t;
   		isect->n = b1 * n1 + b2 * n2 + b3 * n3;
   		isect->primitive = this;
   		isect->bsdf = get_bsdf();
   		```<br>
		8. Return true if an intersection is found; otherwise, return false.<br>
		<br>

		<b>Q3: </b>
		Show images with normal shading for a few small .dae files.
		<br>

		<b>A3: </b>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part1/banana.png" width="400px"/>
				</td>
				<td style="text-align: center;">
				  <img src="part1/CBempty.png" width="400px"/>
				</td>
				<caption>Task 1 and Task 2</caption>
			  </tr> 
			  <tr>
				<td style="text-align: center;">
				  <img src="part1/CBempty_t1p3.png" width="400px"/>
				</td>
				<td style="text-align: center;">
				  <img src="part1/CBempty_t1p4.png" width="400px"/>
				</td>
				<caption>Task 3 and Task 4</caption>
			  </tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		
		<b>Q1: </b>
		Walk through your BVH construction algorithm. 
		Explain the heuristic you chose for picking the splitting point.
		<br>

		<b>A1: </b>
		The BVH (Bounding Volume Hierarchy) construction follows a top-down recursive approach. The algorithm proceeds as follows:  <br>
		1. For each primitive in the scene, I compute its bounding box using the function <code>get_bbox()</code>.
		Then, I create a global bounding box that encompasses all the primitives by iteratively expanding it to include each primitive's bounding box.  
		This global bounding box serves as the root node of the BVH tree. <br>
		2. If the number of primitives in the current node is less than or equal to the given <code>max_leaf_size</code>, the node is designated as a leaf node, and the recursion stops.  
		Otherwise, the node needs to be split into two child nodes to improve spatial partitioning.  <br>
		3. To decide the splitting axis, I consider all three coordinate axes (x, y, and z). 
		For each axis, I calculate the centroid of all primitives' bounding boxes and use it as an approximate midpoint for partitioning.
		I then evaluate the efficiency of splitting along each axis using a heuristic and choose the axis that yields the best partitioning.  
		4. The heuristic I use is inspired by the Surface Area Heuristic (SAH), which aims to minimize the expected cost of ray traversal by balancing the number of primitives and their spatial distribution.  <br>
		The cost function is computed as:  
		\[
		C = NL \times AL + NR \times AR
		\]
		where:  
		- \(NL\) and \(NR\) are the number of primitives in the left and right child nodes, respectively.  
		- \(AL\) and \(AR\) are the surface areas of the bounding boxes of the left and right child nodes, respectively. 
		The goal is to find the axis and split point that minimize \(C\), leading to a more efficient BVH structure. <br>
		5. After determining the best splitting axis, I divide the list of primitives into two halves based on their centroids along the chosen axis.
		6. After splitting the primitives, I recursively call <code>construct_bvh()</code> on the left and right subsets to build the BVH tree Hierarchically. <br>
		7. The process continues until all nodes contain at most <code>max_leaf_size</code> primitives, resulting in a balanced BVH structure that optimizes ray traversal efficiency. <br>
		<br>

		<b>Q2: </b>
		Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
		<br>

		<b>A2: </b>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part2/2_maxplanck.png" width="400px"/>
				</td>
				<td style="text-align: center;">
				  <img src="part2/2_CBlucy.png" width="400px"/>
				</td>
				<caption>Task 1 and Task 2</caption>
			  </tr> 
			  <tr>
				<td style="text-align: center;">
				  <img src="part2/2_CBdragon.png" width="400px"/>
				</td>
				<td style="text-align: center;">
				  <img src="part2/peter.png" width="400px"/>
				</td>
				<caption>large .dae files</caption>
			  </tr>
			</table>
		</div>

		<b>Q3: </b>
		Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
		<br>

		<b>A3: </b>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<th>File Name</th>
					<th>Rendering Time (Without BVH)</th>
					<th>Rendering Time (With BVH)</th>
				</tr>
				<tr>
					<td>cow.dae</td>
					<td>0.1721s</td>
					<td>18.4269s</td>
				</tr>
				<tr>
					<td>beetle.dae</td>
					<td>0.1255s</td>
					<td>28.0541s</td>
				</tr>
				<tr>
					<td>teapot.dae</td>
					<td>0.1333s</td>
					<td>7.7631s</td>
				</tr>
			</table>
		</div>
		Overall, the BVH acceleration significantly improved rendering times for complex geometries.
		For different scenes, the performance gain varied, with some scenes showing a more substantial improvement than others.
		This may be due to the varying complexity and distribution of primitives in each scene.


		<h2>Part 3: Direct Illumination</h2>

		<b>Q1: </b>
		Walk through both implementations of the direct lighting function.
		<br>

		<b>A1: </b> 
		In the function <code>DiffuseBSDF::f()</code>, I use Lambertian reflectance to compute the outgoing radiance for diffuse surfaces. The formula is as follows:
		\[
		f_r(\omega_o, \omega_i) = \frac{\text{albedo}}{\pi}
		\]
		where <code>albedo</code> represents the diffuse reflectance of the surface. 
		<br><br>

		In the function <code>DiffuseBSDF::sample_f()</code>, I use cosine-weighted sampling to generate a random incoming light direction <code>wi</code>. The probability density function (PDF) is computed as:
		\[
		\text{PDF}(\omega_i) = \frac{\cos(\theta)}{\pi}
		\]
		where \(\theta\) is the angle between the surface normal and the incoming direction, corresponding to <code>wi->z</code> in object space. The outgoing radiance is computed using <code>f(wo, *wi)</code>. 
		<br><br>

		In the function <code>zero_bounce_radiance()</code>, I retrieve the emission of the surface using <code>isect.bsdf->get_emission()</code>. 
		<br><br>

		In the function <code>one_bounce_radiance()</code>, I select either <code>estimate_direct_lighting_hemisphere(r, isect)</code> or <code>estimate_direct_lighting_importance(r, isect)</code>, depending on the value of <code>direct_hemisphere_sample</code>. 
		<br><br>

		### **Hemisphere Sampling Method** 
		In <code>estimate_direct_lighting_hemisphere()</code>, I perform the following steps:
		1. Call <code>isect.bsdf->sample_f(w_out, &wi_d, &pdf)</code> multiple times (equal to <code>num_samples</code>).
		2. Compute <code>cosTheta = dot(wi_d, Vector3D(0, 0, 1))</code> in object space.
		3. Construct the next ray <code>r_next</code> with:
		   - Origin: <code>hit_p</code> (current intersection point).
		   - Direction: <code>wi_d</code> (sampled incoming light direction).
		4. If the next intersection is a light source, compute its radiance using <code>isect.bsdf->get_emission()</code> and accumulate the result.
		5. Finally, normalize the accumulated result by dividing it by <code>num_samples</code>. 
		<br><br>

		### **Importance Sampling Method** 
		In <code>estimate_direct_lighting_importance()</code>, I iterate over all the lights in the scene using:
		<pre><code>for (auto light = scene->lights.begin(); light != scene->lights.end(); light++)</code></pre>
		For each light source:
		1. Sample the light using:
		   <pre><code>Vector3D L = light->sample_L(hit_p, &wi_d, &distToLight, &pdf);</code></pre>
		2. Compute <code>cosTheta = dot(wi_d, isect.n)</code>.
		3. Construct the shadow ray <code>r_next</code>:
		   - Origin: <code>hit_p</code>.
		   - Direction: <code>wi_d</code>.
		   - Set <code>r_next.max_t = (double)distToLight - EPS_F</code> to ensure the ray does not extend beyond the light source.
		4. If the light is not blocked, compute the radiance using Monte Carlo integration:
		   <pre><code>L_light += L * f * cosTheta / pdf;</code></pre>
		5. If the light source is a delta light (such as a point light), only one sample is needed instead of multiple samples.
		6. Finally, normalize the accumulated result by the number of samples.
		<br>

		In the function <code>est_radiance_global_illumination()</code>, I add <code>zero_bounce_radiance()</code> and <code>one_bounce_radiance()</code> to compute the total radiance.

		<br>
		Overall, importance sampling improves efficiency by focusing samples only in the directions where light actually comes from, reducing variance compared to uniform hemisphere sampling.

		<b>Q2: </b>
		Show some images rendered with both implementations of the direct lighting function.
		<br>

		<b>A2: </b>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/33_CBbunny_H_16_8.png" width="400px"/>
				  <figcaption>Hemisphere Sampling</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/34_bunny_1_1.png" width="400px"/>
				  <figcaption>Importance Sampling</figcaption>
				</td>
			  </tr> 
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/CBbunny_H_64_32.png" width="400px"/>
				  <figcaption>Hemisphere Sampling</figcaption>
				</td>
				<td>
					<img src="part3/3_bunny_64_32.png" width="400px"/>
					<figcaption>Importance Sampling</figcaption>
				</td>
			  </tr>
			  <tr>
				<td>
					<img src="part3/ww_CBsphere_lam_H_64_32.png" width="400px"/>
					<figcaption>Hemisphere Sampling</figcaption>
				</td>
				<td>
					<img src="part3/ww_CBsphere_lam_64_32.png" width="400px"/>
					<figcaption>Importance Sampling</figcaption>	
				</td>
			  </tr>
			</table>
		</div>

		<b>Q3: </b>
		Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
		<br>

		<b>A3: </b>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/35_CBbunny_1_1.png" width="400px"/>
				  <figcaption>1 light ray, 1 sample per pixel</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/35_CBbunny_4_1.png" width="400px"/>
				  <figcaption>4 light rays, 1 sample per pixel</figcaption>
				</td>
			  </tr> 
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/35_CBbunny_16_1.png" width="400px"/>
				  <figcaption>16 light rays, 1 sample per pixel</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/35_CBbunny_64_1.png" width="400px"/>
				  <figcaption>64 light rays, 1 sample per pixel</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		Point light sources cause hard shadows, while area light sources create soft shadows.
		We control the number of samples per pixel to ensure that the noise level is only affected by the number of light rays, not pixel sampling.
		As we can see from the images, as the number of light rays increases, the noise level decreases significantly. This is because more light rays provide more information about the light distribution, resulting in smoother and more accurate soft shadows.
		<br>

		<b>Q4: </b>
		Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
		<br>

		<b>A4: </b>
		In our comparison between uniform hemisphere sampling and light sampling, we observed that uniform hemisphere sampling produces significantly more noise and takes longer to render. 
		The primary reason for the higher noise is that uniform hemisphere sampling distributes rays evenly in all directions, leading to many samples that contribute little to direct illumination, especially when the area light is relatively small. 
		This results in inefficient sampling, requiring more rays to converge to a smooth shadow. 
		On the other hand, light sampling directs rays preferentially toward the light source, ensuring that more samples contribute meaningfully to the illumination calculation. 
		This improves convergence, reducing noise with fewer samples. 
		Additionally, because fewer rays are wasted on unimportant directions, light sampling achieves a shorter render time compared to uniform hemisphere sampling while also producing smoother soft shadows.

		<h2>Part 4: Global Illumination</h2>

		<b>Q1: </b>
		Walk through your implementation of the indirect lighting function.
		<br>


		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>